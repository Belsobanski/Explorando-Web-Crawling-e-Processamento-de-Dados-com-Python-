# Projeto Web Crawling e Processamento de Dados com Python

Este projeto realiza a extração de notícias do site da **CNN** utilizando **web crawling**. Os dados extraídos incluem informações como **Título**, **Data de Publicação** e **Link** das notícias, e são salvos em dois formatos: **JSON**, **CSV** e **Parquet** para futuras análises.

## Tecnologias Utilizadas

- **Python**: Linguagem de programação principal utilizada para o desenvolvimento do código.
- **BeautifulSoup**: Biblioteca utilizada para realizar o scraping das páginas web.
- **Pandas**: Biblioteca para manipulação de dados e transformação para formatos CSV e Parquet.
- **JSON**: Formato de armazenamento dos dados extraídos para fácil manipulação.
- **Parquet**: Formato de armazenamento otimizado para grandes volumes de dados, adequado para Big Data.
- **PyArrow**: Biblioteca necessária para trabalhar com arquivos Parquet no Python.

## Objetivo

O objetivo deste projeto é demonstrar o uso de **web scraping** para extrair dados de uma página web (neste caso, as notícias da CNN) e armazená-los de maneira eficiente em arquivos **JSON**, **CSV** e **Parquet**. Essa abordagem permite uma análise mais flexível e rápida dos dados extraídos.


